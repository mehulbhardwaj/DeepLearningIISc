{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mehulbhardwaj/DeepLearningIISc/blob/main/Copy_of_Cohort_6_Team_5_M4_Mini_Hackathon_To_Classify_Coronavirus_Tweets_During_Covid_19.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "\n",
        "### Mini Project Notebook: To perform text classification of coronavirus tweets during the peak Covid - 19 period using LSTMs/RNNs/CNNs/BERT.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maritime-miami"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nljJR6CwfZN_"
      },
      "source": [
        "At the end of the mini-hackathon, you will be able to :\n",
        "\n",
        "* perform data preprocessing/preprocess the text\n",
        "* represent the text/words using the pretrained word embeddings - Word2Vec/Glove\n",
        "* build the deep neural network (RNN, LSTM, GRU, CNNs, Bidirectional-LSTM, GRU, BERT) to classify the tweets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction\n",
        "\n",
        "First we need to understand why sentiment analysis is needed for social media?\n",
        "\n",
        "People from all around the world have been using social media more than ever. Sentiment analysis on social media data helps to understand the wider public opinion about certain topics such as movies, events, politics, sports, and more and gain valuable insights from this social data. Sentiment analysis has some powerful applications. Nowadays it is also used by some businesses to do market research and understand the customer’s experiences for their products or services.\n",
        "\n",
        "Now an interesting question about this type of problem statement that may arise in your mind is that why sentiment analysis on COVID-19 Tweets? What is about the coronavirus tweets that would be positive? You may have heard sentiment analysis on movie or book reviews, but what is the purpose of exploring and analyzing this type of data?\n",
        "\n",
        "The use of social media for communication during the time of crisis has increased remarkably over the recent years. As mentioned above, analyzing social media data is important as it helps understand public sentiment. During the coronavirus pandemic, many people took to social media to express their anger, grief, or sadness while some also spread happiness and positivity. People also used social media to ask their network for help related to vaccines or hospitals during this hard time. Many issues related to this pandemic can also be solved if experts considered this social data. That’s the reason why analyzing this type of data is important to understand the overall issues faced by people.\n",
        "\n"
      ],
      "metadata": {
        "id": "iNI_-0spy1Ho"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL0Ve1abn6YJ"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The given challenge is to build a multiclass classification model to predict the sentiment of Covid-19 tweets. The tweets have been pulled from Twitter and manual tagging has been done. We are given information like Location, Tweet At, Original Tweet, and Sentiment.\n",
        "\n",
        "The training dataset consists of 36000 tweets and the testing dataset consists of 8955 tweets. There are 5 sentiments namely ‘Positive’, ‘Extremely Positive’, ‘Negative’, ‘Extremely Negative’, and ‘Neutral’ in the sentiment column.\n",
        "\n",
        "## Description\n",
        "\n",
        "This dataset has the following information about the user who tweeted:\n",
        "\n",
        "1. **UserName:** twitter handler\n",
        "2. **ScreenName:** a personal identifier on Twitter and is separate from the username\n",
        "3. **Location:** where in the world the person tweets from\n",
        "4. **TweetAt:** date of the tweet posted (DD-MM-YYYY)\n",
        "5. **OriginalTweet:** the tweet itself\n",
        "6. **Sentiment:** sentiment value\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih-oasWmdZul"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfWGmjNHdZul"
      },
      "source": [
        "To build and implement a multiclass classification deep neural network model to classify between Positive/Extremely Positive/Negative/Extremely Negative/Neutral sentiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BQEA97zTlTa"
      },
      "source": [
        "## Grading = 10 Marks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdYmy-tJgURN"
      },
      "source": [
        "Here is a handy link to Kaggle's competition documentation (https://www.kaggle.com/docs/competitions), which includes, among other things, instructions on submitting predictions (https://www.kaggle.com/docs/competitions#making-a-submission)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8OapRtHgLnU"
      },
      "source": [
        "## Instructions for downloading train and test dataset from Kaggle API are as follows:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2jS73oLnCR"
      },
      "source": [
        "### 1. Create an API key in Kaggle.\n",
        "\n",
        "To do this, go to the competition site on Kaggle at (https://www.kaggle.com/t/15cef0def403469ebbb5db1a67991873) and open your user settings page. Click Account.\n",
        "\n",
        "* Click on your profile picture at the top-right corner of the page.\n",
        "\n",
        "![alt text](https://i.imgur.com/kSLmEj2.png)\n",
        "\n",
        "* In the popout menu, click the Settings option.\n",
        "\n",
        "![alt text](https://i.imgur.com/tNi6yun.png)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkzGffHdbwX2"
      },
      "source": [
        "### 2. Next, scroll down to the API access section and click generate to download an API key (kaggle.json).\n",
        "![alt text](https://i.imgur.com/vRNBgrF.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtETuXx8b-OC"
      },
      "source": [
        "### 3. Upload your kaggle.json file using the following snippet in a code cell:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1pfXBDxWl0Y",
        "collapsed": true
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCV_T6MMW4eX"
      },
      "source": [
        "#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n",
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbukdzJ6cE32"
      },
      "source": [
        "### 4. Install the Kaggle API using the following command\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMj1n1MJcqzN"
      },
      "source": [
        "!pip install -U -q kaggle\n",
        "!pip install transformers==4.28.1\n",
        "!pip install scikit-learn\n",
        "!pip install imbalanced-learn\n",
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vpy9P1nchhd"
      },
      "source": [
        "### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQbPsDOLZ0b4"
      },
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BenAWlpI73sm"
      },
      "source": [
        "# Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n",
        "!ls ~/.kaggle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lart ~/.kaggle"
      ],
      "metadata": {
        "id": "9P8JBOJL4G_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm2jGsCradOS"
      },
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json # run this command to ensure your Kaggle API token is secure on colab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32unPZKzdI72"
      },
      "source": [
        "### 6. Now download the Test Data from Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppuy5gRKHFwv"
      },
      "source": [
        "**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41-ETZCE_A1j"
      },
      "source": [
        "If you encounter **401-unauthorised** download latest **kaggle.json** by repeating steps 1 & 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY40TmgfHq0s"
      },
      "source": [
        "#If you get a forbidden link, you have most likely not joined the competition.\n",
        "!kaggle competitions download -c to-classify-coronavirus-tweets-during-covid-19"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/to-classify-coronavirus-tweets-during-covid-19.zip"
      ],
      "metadata": {
        "id": "mvKRiFNglvpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## YOUR CODING STARTS FROM HERE"
      ],
      "metadata": {
        "id": "QeKon2vruI_c"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abstract-stocks"
      },
      "source": [
        "## Import required packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Import required packages\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import emoji\n",
        "import html\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, f1_score, precision_score, recall_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import TrainerCallback, AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
        "from imblearn.over_sampling import RandomOverSampler  # Import RandomOverSampler\n",
        "import json  # For saving metrics\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "\n",
        "from scipy.stats import chi2_contingency"
      ],
      "metadata": {
        "id": "O5RcxwQUku6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53g0zVbjRV7K"
      },
      "source": [
        "##   **Stage 1**:  Data Loading and Perform Exploratory Data Analysis (1 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Load the Dataset\n"
      ],
      "metadata": {
        "id": "xIa9LlhMHj5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Load the training dataset\n",
        "train_df = pd.read_csv('corona_nlp_train.csv/corona_nlp_train.csv', encoding='latin1')\n",
        "\n",
        "# Load the testing dataset\n",
        "test_df = pd.read_csv('corona_nlp_test.csv/corona_nlp_test.csv', encoding='latin1')\n",
        "\n",
        "train_df.info()\n",
        "test_df.info()\n",
        "\n"
      ],
      "metadata": {
        "id": "6qmR5Vo_tbVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Check for Missing Values"
      ],
      "metadata": {
        "id": "hzQS91rfJLNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "# Check for missing values in the training dataset\n",
        "train_missing_values = train_df.isnull().sum()\n",
        "print(\"Missing values in training dataset:\\n\", train_missing_values)\n",
        "\n",
        "# Check for missing values in the testing dataset\n",
        "test_missing_values = test_df.isnull().sum()\n",
        "print(\"\\nMissing values in testing dataset:\\n\", test_missing_values)\n"
      ],
      "metadata": {
        "id": "JF3xCD9qJN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill the missing localtions using the distributon in rest of the data."
      ],
      "metadata": {
        "id": "d_qjODcCtiPs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check relation between location and sentiment\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(x='Location', hue='Sentiment', data=train_df, order=train_df['Location'].value_counts().index[:10])\n",
        "plt.title(\"Sentiment Distribution by Location\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(train_df['Location'], train_df['Sentiment'])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2}, P-value: {p}\")"
      ],
      "metadata": {
        "id": "oWYYtHNpYPg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check relation between TweetAt and sentiment\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(train_df['TweetAt'], train_df['Sentiment'])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2}, P-value: {p}\")"
      ],
      "metadata": {
        "id": "vKheDFtHMZK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check relation between ScreenName and sentiment\n",
        "# Create a contingency table\n",
        "contingency_table = pd.crosstab(train_df['UserName'], train_df['Sentiment'])\n",
        "\n",
        "# Perform chi-square test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "print(f\"Chi-Square Statistic: {chi2}, P-value: {p}\")"
      ],
      "metadata": {
        "id": "Kjs6N8tbMvdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use Sentiment-Weighted Probabilistic Assignment for filling missing Location values.\n",
        "\n",
        "# Step 1: Calculate sentiment-specific probability distribution of locations\n",
        "location_probs = (\n",
        "    train_df.groupby('Sentiment')['Location']\n",
        "    .value_counts(normalize=True)\n",
        "    .rename(\"probability\")\n",
        "    .reset_index()\n",
        ")\n",
        "\n",
        "# Step 2: Calculate the overall probability distribution of locations\n",
        "overall_location_probs = (\n",
        "    train_df['Location'].value_counts(normalize=True)\n",
        "    .rename(\"probability\")\n",
        "    .reset_index()\n",
        "    .rename(columns={'index': 'Location'})\n",
        ")\n",
        "\n",
        "# Step 3: Define a function to sample a location by sentiment-specific probabilities\n",
        "def sample_location_by_sentiment(sentiment):\n",
        "    # Get probabilities for the given sentiment\n",
        "    sentiment_probs = location_probs[location_probs['Sentiment'] == sentiment]\n",
        "    locations = sentiment_probs['Location'].values\n",
        "    probabilities = sentiment_probs['probability'].values\n",
        "\n",
        "    # If no probabilities are available (edge case), fallback to overall probabilities\n",
        "    if len(locations) == 0:\n",
        "        return np.random.choice(overall_location_probs['Location'], p=overall_location_probs['probability'])\n",
        "\n",
        "    # Sample a location weighted by probabilities\n",
        "    return np.random.choice(locations, p=probabilities)\n",
        "\n",
        "# Step 4: Fill missing locations in train_df using sentiment-specific probabilities\n",
        "train_df['Location'] = train_df.apply(\n",
        "    lambda row: sample_location_by_sentiment(row['Sentiment']) if pd.isna(row['Location']) else row['Location'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "# Step 5: Fill missing locations in test_df using overall probabilities\n",
        "test_df['Location'] = test_df['Location'].fillna(\n",
        "    pd.Series(np.random.choice(overall_location_probs['Location'],\n",
        "                                p=overall_location_probs['probability'],\n",
        "                                size=len(test_df)))\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "# Calculate the probability distribution of locations\n",
        "location_probs = train_df['Location'].value_counts(normalize=True)\n",
        "\n",
        "# Get the locations and their probabilities\n",
        "locations = location_probs.index\n",
        "probs = location_probs.values\n",
        "\n",
        "# Fill missing values in training and testing data\n",
        "train_df['Location'] = train_df['Location'].fillna(pd.Series(np.random.choice(locations,\n",
        "                                                                            p=probs,\n",
        "                                                                            size=len(train_df))))\n",
        "test_df['Location'] = test_df['Location'].fillna(pd.Series(np.random.choice(locations,\n",
        "                                                                           p=probs,\n",
        "                                                                           size=len(test_df))))\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "5RmSp-sXMSfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Calculate the probability distribution of locations\n",
        "location_probs = train_df['Location'].value_counts(normalize=True)\n",
        "\n",
        "# Get the locations and their probabilities\n",
        "locations = location_probs.index\n",
        "probs = location_probs.values\n",
        "\n",
        "# Fill missing values in training and testing data\n",
        "num_missing_train = train_df['Location'].isnull().sum()\n",
        "random_locations_train = pd.Series(np.random.choice(locations, p=probs, size=num_missing_train))\n",
        "train_df['Location'] = train_df['Location'].fillna(random_locations_train)\n",
        "\n",
        "num_missing_test = test_df['Location'].isnull().sum()\n",
        "random_locations_test = pd.Series(np.random.choice(locations, p=probs, size=num_missing_test))\n",
        "test_df['Location'] = test_df['Location'].fillna(random_locations_test)\n"
      ],
      "metadata": {
        "id": "Q0wgHNE8tSiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check for Missing Values"
      ],
      "metadata": {
        "id": "kXJBnKGLt8SV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# YOUR CODE HERE\n",
        "# Check for missing values in the training dataset\n",
        "train_missing_values = train_df.isnull().sum()\n",
        "print(\"Missing values in training dataset:\\n\", train_missing_values)\n",
        "\n",
        "# Check for missing values in the testing dataset\n",
        "test_missing_values = test_df.isnull().sum()\n",
        "print(\"\\nMissing values in testing dataset:\\n\", test_missing_values)\n"
      ],
      "metadata": {
        "id": "lWJoCIXut-MJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize the sentiment column values\n"
      ],
      "metadata": {
        "id": "nra2K6EPHosw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# YOUR CODE HERE\n",
        "sentiment_counts = train_df['Sentiment'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
        "plt.title('Distribution of Sentiments')\n",
        "plt.xlabel('Sentiment')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()  # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5ksnP-I2Fitd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Visualize top 10 Countries that had the highest tweets using countplot (Tweet count vs Location)\n"
      ],
      "metadata": {
        "id": "_zc6AUq9Hry8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Get the top 10 locations\n",
        "top_10_locations = train_df['Location'].value_counts().nlargest(10).index\n",
        "\n",
        "# Filter the dataframe for the top 10 locations\n",
        "filtered_df = train_df[train_df['Location'].isin(top_10_locations)]\n",
        "\n",
        "# Create the countplot\n",
        "plt.figure(figsize=(12, 6))  # Adjust figure size as needed\n",
        "sns.countplot(data=filtered_df, x='Location', order=top_10_locations)\n",
        "plt.title('Top 10 Countries with Highest Tweet Counts')\n",
        "plt.xlabel('Location')\n",
        "plt.ylabel('Tweet Count')\n",
        "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M_fUPMJzGl8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Plotting Pie Chart for the Sentiments in percentage\n"
      ],
      "metadata": {
        "id": "GUIM_P-VHuzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Calculate sentiment percentages\n",
        "sentiment_counts = train_df['Sentiment'].value_counts()\n",
        "sentiment_percentages = sentiment_counts / sentiment_counts.sum() * 100\n",
        "\n",
        "# Create the pie chart\n",
        "plt.figure(figsize=(8, 8))  # Adjust figure size as needed\n",
        "plt.pie(sentiment_percentages, labels=sentiment_percentages.index, autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Distribution of Sentiments in Percentage')\n",
        "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s8oRZOYDHAVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* WordCloud for the Tweets/Text\n",
        "\n",
        "    * Visualize the most commonly used words in each sentiment using wordcloud\n",
        "    * Refer to the following [link](https://medium.com/analytics-vidhya/word-cloud-a-text-visualization-tool-fb7348fbf502) for Word Cloud: A Text Visualization tool\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cSvzz5z6H8kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine all tweets for each sentiment\n",
        "sentiment_texts = {sentiment: ' '.join(train_df[train_df['Sentiment'] == sentiment]['OriginalTweet'].astype(str).tolist())\n",
        "                  for sentiment in train_df['Sentiment'].unique()}\n",
        "\n",
        "# Add custom words to ignore\n",
        "#custom_stopwords = ['COVID', 'https', 't', 'co', 'coronavirus','COVID19']\n",
        "#STOPWORDS.update(custom_stopwords)\n",
        "\n",
        "# Create and display word clouds for each sentiment\n",
        "for sentiment, text in sentiment_texts.items():\n",
        "    # Create a word cloud object\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
        "                          stopwords=STOPWORDS, min_font_size=10).generate(text)\n",
        "\n",
        "    # Display the generated image\n",
        "    plt.figure(figsize=(8, 8), facecolor=None)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.title(f\"Word Cloud for {sentiment} Sentiment\")\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "vq7__byEHabv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oLyIb19KcdL"
      },
      "source": [
        "##   **Stage 2**: Data Pre-Processing  (2 Points)\n",
        "\n",
        "####  Clean and Transform the data into a specified format\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "#\n",
        "#\n",
        "#def define_common_words_tfidf(df, text_column='OriginalTweet', sentiment_column='Sentiment', threshold=0.1):\n",
        "\"\"\"\n",
        "    Identifies common words with low TF-IDF scores across documents for different sentiment classes.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame containing text and sentiment columns.\n",
        "        text_column (str): The name of the column containing the text data (default: 'OriginalTweet').\n",
        "        sentiment_column (str): The name of the column containing the sentiment labels (default: 'Sentiment').\n",
        "        threshold (float): Threshold for identifying low-importance words (default: 0.1).\n",
        "\n",
        "    Returns:\n",
        "        list: A list of common words to exclude for training.\n",
        "\"\"\"\n",
        "    # Combine all text by sentiment category\n",
        "#    df_grouped = df.groupby(sentiment_column)[text_column].apply(lambda x: ' '.join(x)).reset_index()\n",
        "\n",
        "    # Vectorize using TF-IDF\n",
        "#    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "#    tfidf_matrix = vectorizer.fit_transform(df_grouped[text_column])\n",
        "#    words = vectorizer.get_feature_names_out()\n",
        "#    avg_tfidf_scores = tfidf_matrix.mean(axis=0).A1  # Average TF-IDF score for each word across sentiments\n",
        "\n",
        "    # Identify low-importance words based on the threshold\n",
        "#    common_words = [word for word, score in zip(words, avg_tfidf_scores) if score < 0.1]\n",
        "\n",
        "#    return common_words\n",
        "\n",
        "#common_words = define_common_words(train_df)  # Identify common words to remove from data\n",
        "#print(common_words)\n"
      ],
      "metadata": {
        "id": "GPYs2XALE-5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK resources if you haven't already\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "#common_words = ['COVID', 'https', 't', 'co', 'coronavirus', 'COVID19']\n",
        "\n",
        "def split_hashtag(hashtag):\n",
        "    return ' '.join(re.findall(r'[A-Z][^A-Z]*', hashtag))\n",
        "\n",
        "def process_hashtags(text):\n",
        "    \"\"\"\n",
        "    Processes hashtags by splitting camel case and converting to lowercase.\n",
        "    \"\"\"\n",
        "    return re.sub(r'#(\\w+)', lambda match: split_hashtag(match.group(1)).lower(), text)\n",
        "\n",
        "# Define emoticon dictionary\n",
        "emoticon_dict = {\n",
        "    \":)\": \"positive_smile\",\n",
        "    \":-)\": \"positive_smile\",\n",
        "    \":(\": \"negative_sad\",\n",
        "    \":-(\": \"negative_sad\",\n",
        "    \";)\": \"positive_wink\",\n",
        "    \";-)\": \"positive_wink\",\n",
        "}\n",
        "\n",
        "def convert_emojis(text):\n",
        "    \"\"\"\n",
        "    Converts emojis to their descriptive text.\n",
        "    \"\"\"\n",
        "    return emoji.demojize(text)\n",
        "\n",
        "def convert_emoticons(text):\n",
        "    \"\"\"\n",
        "    Converts emoticons to sentiment tokens.\n",
        "    \"\"\"\n",
        "    for emoticon, sentiment in emoticon_dict.items():\n",
        "        text = text.replace(emoticon, sentiment)\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove mentions\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        "\n",
        "    # Process hashtags\n",
        "    text = process_hashtags(text)\n",
        "\n",
        "    # Handle HTML ampersands\n",
        "    text = html.unescape(text)\n",
        "\n",
        "    # Convert emojis and emoticons\n",
        "    text = convert_emojis(text)\n",
        "    text = convert_emoticons(text)\n",
        "\n",
        "    # Remove non-ASCII characters (but keep emojis processed above)\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "\n",
        "    # Replace multiple spaces with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Remove non-alphanumeric characters except spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Function to remove duplicate words\n",
        "    text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    #remove all 1 char words\n",
        "    text = \" \".join(re.findall(r'\\b\\w{2,}\\b', text))\n",
        "\n",
        "    # Replace COVID-related terms with \"coronavirus\"\n",
        "    patterns = r\"covid[_\\-]19|covid19|covid 19|covid\"\n",
        "    text = re.sub(patterns, \"coronavirus\", text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function to the 'OriginalTweet' column in both DataFrames\n",
        "train_df['CleanedTweet'] = train_df['OriginalTweet'].apply(clean_text)\n",
        "test_df['CleanedTweet'] = test_df['OriginalTweet'].apply(clean_text)"
      ],
      "metadata": {
        "id": "-6ZCiIxxKiq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['CleanedTweet'].head()"
      ],
      "metadata": {
        "id": "XPa916HV_b2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" commented out as it requires higher RAM to run\n",
        "#Given high correlation between location and sentiment, and 18% dataset missing location values, we will train a model to fill missing location values\n",
        "\n",
        "# Split dataset into known and missing locations\n",
        "known_locations = train_df[train_df['Location'].notna()]\n",
        "missing_locations = train_df[train_df['Location'].isna()]\n",
        "\n",
        "\n",
        "# Vectorize the text data\n",
        "vectorizer = CountVectorizer(max_features=2000)  # Limit to top 2000 words\n",
        "X_known = vectorizer.fit_transform(known_locations['CleanedTweet'])\n",
        "y_known = known_locations['Location']\n",
        "\n",
        "# Add sentiment as an additional feature\n",
        "sentiment_mapping = {'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3, 'Extremely Positive': 4}\n",
        "sentiment_known = known_locations['Sentiment'].map(sentiment_mapping).values.reshape(-1, 1)\n",
        "X_known = np.hstack((X_known.toarray(), sentiment_known))  # Combine text and sentiment\n",
        "\n",
        "# Split into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_known, y_known, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Random Forest model\n",
        "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "\n",
        "\n",
        "y_pred = rf_model.predict(X_val)\n",
        "print(\"Accuracy on Validation Set:\", accuracy_score(y_val, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "6Hpus6SEaInm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" commented out as it requires higher RAM to run\n",
        "# Predict missing location values for the training dataset\n",
        "\n",
        "# Vectorize the text data for missing locations\n",
        "X_missing = vectorizer.transform(missing_locations['CleanedTweet'])\n",
        "\n",
        "# Add sentiment as an additional feature\n",
        "sentiment_missing = missing_locations['Sentiment'].map(sentiment_mapping).values.reshape(-1, 1)\n",
        "X_missing = np.hstack((X_missing.toarray(), sentiment_missing))  # Combine text and sentiment\n",
        "\n",
        "# Predict missing locations\n",
        "predicted_locations = rf_model.predict(X_missing)\n",
        "\n",
        "# Fill missing values in the original dataframe\n",
        "train_df.loc[train_df['Location'].isna(), 'Location'] = predicted_locations\n",
        "\n",
        "# Check the distribution of location after filling missing values\n",
        "print(train_df['Location'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "Sc7xEsoZbP6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" commented out as it requires higher RAM to run\n",
        "# Predict missing location values in test_df\n",
        "\n",
        "# Filter rows with missing locations in test_df\n",
        "missing_test_locations = test_df[test_df['Location'].isna()]\n",
        "\n",
        "# Vectorize the text data for missing locations in test_df\n",
        "X_missing_test = vectorizer.transform(missing_test_locations['CleanedTweet'])\n",
        "\n",
        "# Add sentiment as an additional feature\n",
        "sentiment_missing_test = missing_test_locations['Sentiment'].map(sentiment_mapping).values.reshape(-1, 1)\n",
        "X_missing_test = np.hstack((X_missing_test.toarray(), sentiment_missing_test))  # Combine text and sentiment\n",
        "\n",
        "# Predict missing locations\n",
        "predicted_test_locations = rf_model.predict(X_missing_test)\n",
        "\n",
        "# Fill missing values in the original test_df\n",
        "test_df.loc[test_df['Location'].isna(), 'Location'] = predicted_test_locations\n",
        "\n",
        "# Check the distribution of location after filling missing values in test_df\n",
        "print(test_df['Location'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "nE67k6NddnkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Combine text, TweetAt and location into a single feature\n",
        "train_df['text_with_location'] = train_df['CleanedTweet'] + \" Location: \" + train_df['Location'] + \" TweetAt: \" + train_df['TweetAt']\n",
        "test_df['text_with_location'] = test_df['CleanedTweet'] + \" Location: \" + test_df['Location'] + \" TweetAt: \" + train_df['TweetAt']\n"
      ],
      "metadata": {
        "id": "DhtyWuyZe8Ga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualise cleanded word clouds\n",
        "# Combine all tweets for each sentiment\n",
        "sentiment_texts = {sentiment: ' '.join(train_df[train_df['Sentiment'] == sentiment]['CleanedTweet'].astype(str).tolist())\n",
        "                  for sentiment in train_df['Sentiment'].unique()}\n",
        "\n",
        "# Create and display word clouds for each sentiment\n",
        "for sentiment, text in sentiment_texts.items():\n",
        "    # Create a word cloud object\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
        "                          stopwords=STOPWORDS, min_font_size=10).generate(text)\n",
        "\n",
        "    # Display the generated image\n",
        "    plt.figure(figsize=(8, 8), facecolor=None)\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis(\"off\")\n",
        "    plt.tight_layout(pad=0)\n",
        "    plt.title(f\"Word Cloud for {sentiment} Sentiment\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "wyiroDYO_EwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KeZKtwMle6dN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Cohort6_Team5_Hackathon1_CoronavirusClassification_Models/cleaned_data_train.csv\"\n",
        "train_df.to_csv(file_path, index=False)\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Cohort6_Team5_Hackathon1_CoronavirusClassification_Models/cleaned_data_test.csv\"\n",
        "test_df.to_csv(file_path, index=False)\n"
      ],
      "metadata": {
        "id": "aqpMjOjTeNKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jZg7yL2TtTM"
      },
      "source": [
        "##   **Stage 3**: Build the Word Embeddings using pretrained Word2vec/Glove (Text Representation) (1 Point)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -qq https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "GloVe_Dict_200d = {}\n",
        "# Loading the 200-dimensional vector of the model\n",
        "with open(\"glove.6B.200d.txt\", 'r') as f:\n",
        "  for line in f:\n",
        "      values = line.split()\n",
        "      word = values[0]\n",
        "      vector = np.asarray(values[1:], \"float32\")\n",
        "      GloVe_Dict_200d[word] = vector\n",
        "\n",
        "\n",
        "def glove_embeddings(text,dim ):\n",
        "    if len(text) < 1:\n",
        "        return np.zeros(dim)\n",
        "    else:\n",
        "        vectorized = [GloVe_Dict_200d[word] if word in GloVe_Dict_200d else np.random.randn(dim) for word in text]\n",
        "    sum = np.sum(vectorized,axis=0)\n",
        "    # Return the average\n",
        "    return sum/len(text)\n",
        "\n",
        "\n",
        "def get_glove_embeddings(text,dimension):\n",
        "        embeddings = text.apply(lambda x: glove_embeddings(x,dimension))\n",
        "        return list(embeddings)\n",
        "\n",
        "glove_embeddings_train = get_glove_embeddings(train_df['CleanedTweet'],dimension=200)\n",
        "glove_embeddings_test = get_glove_embeddings(test_df['CleanedTweet'],dimension=200)\n"
      ],
      "metadata": {
        "id": "FEBRlyrAEAv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_embeddings_train[0]"
      ],
      "metadata": {
        "id": "sZI7SN_VRHoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6jfm3YFUL7i"
      },
      "source": [
        "##   **Stage 4**: Build and Train the Deep Recurrent Model using Pytorch/Keras (4 Points)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def word_statistics(dataframe, text_column):\n",
        "  \"\"\"Calculates word statistics for sentences in a dataframe's text column.\n",
        "\n",
        "  Args:\n",
        "    dataframe: The input Pandas DataFrame.\n",
        "    text_column: The name of the column containing text data.\n",
        "\n",
        "  Returns:\n",
        "    A tuple containing the mean, 75th percentile, and 95th percentile\n",
        "    of the number of words in sentences.\n",
        "  \"\"\"\n",
        "  word_counts = []\n",
        "  for text in dataframe[text_column]:\n",
        "      sentences = text.split(\".\")  # Split the text into sentences\n",
        "      word_counts.extend([len(sentence.split()) for sentence in sentences if sentence])  # Count words in each sentence\n",
        "\n",
        "  mean_words = np.mean(word_counts)\n",
        "  percentile_75 = np.percentile(word_counts, 75)\n",
        "  percentile_95 = np.percentile(word_counts, 95)\n",
        "  percentile_99 = np.percentile(word_counts, 99)\n",
        "  return mean_words, percentile_75, percentile_99\n",
        "\n",
        "\n",
        "# Calculate and print the word statistics for the training set\n",
        "mean_words, percentile_75, percentile_95 = word_statistics(train_df, \"text_with_location\")  # Assuming \"text\" column contains the text data\n",
        "\n",
        "print(f\"Mean words per sentence in training set: {mean_words}\")\n",
        "print(f\"75th percentile of word count in training set: {percentile_75}\")\n",
        "print(f\"99th percentile of word count in training set: {percentile_95}\")"
      ],
      "metadata": {
        "id": "LIQdP_UmQ_c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Step 1: Mount Google Drive and configure base model\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path in Google Drive where you want to save the best model\n",
        "best_model_path = \"/content/drive/MyDrive/Cohort6_Team5_Hackathon1_CoronavirusClassification_Models/BERTweet_best_model\"\n",
        "\n",
        "\n",
        "# Use BERTweet model and tokenizer\n",
        "model_name = \"vinai/bertweet-base\"  # BERTweet model for first training\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, normalization=True)\n",
        "\n",
        " # Step 2: Pick base model or your own pre trained model\n",
        "# for first training\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=5)\n",
        "\n",
        "#for finetuning pre-trained model\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
        "\n",
        "# Callback to print F1 Score every 100 iterations and accuracy after each epoch\n",
        "class MetricsCallback(TrainerCallback):\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        # Print F1 score every 100 steps\n",
        "        if state.global_step % 100 == 0 and state.global_step > 0:\n",
        "            predictions = trainer.predict(val_dataset)\n",
        "            preds = predictions.predictions.argmax(-1)\n",
        "            f1 = f1_score(val_labels, preds, average='weighted')\n",
        "            print(f\"Step {state.global_step}: Validation F1 Score = {f1:.4f}\")\n",
        "\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        # Calculate accuracy after each epoch\n",
        "        predictions = trainer.predict(val_dataset)\n",
        "        preds = predictions.predictions.argmax(-1)\n",
        "        acc = accuracy_score(val_labels, preds)\n",
        "        print(f\"Epoch {state.epoch}: Validation Accuracy = {acc:.4f}\")\n",
        "\n",
        "\n",
        "# Add a SaveMetricsCallback that logs metrics (e.g., F1 score, accuracy) to a JSON file (metrics.json) on Google Drive after every evaluation.\n",
        "class SaveMetricsCallback(TrainerCallback):\n",
        "    def on_evaluate(self, args, state, control, metrics=None, **kwargs):\n",
        "        log_path = \"/content/drive/MyDrive/Cohort6_Team5_Hackathon1_CoronavirusClassification_Models/metrics.json\"\n",
        "        metrics = metrics or {}\n",
        "        with open(log_path, 'a') as f:\n",
        "            f.write(json.dumps({'epoch': state.epoch, 'metrics': metrics}) + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "# Custom dataset class\n",
        "class TweetDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = torch.tensor(labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# Prepare data\n",
        "train_texts = train_df['text_with_location'].tolist()\n",
        "train_labels = train_df['Sentiment'].map({'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3, 'Extremely Positive': 4}).tolist()\n",
        "test_texts = test_df['CleanedTweet'].tolist()\n",
        "test_labels = train_df['text_with_location'].map({'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3, 'Extremely Positive': 4}).tolist()\n",
        "\n",
        "# Step 3: Balance sentiment classes\n",
        "#ros = RandomOverSampler(random_state=42)\n",
        "#train_texts_resampled, train_labels_resampled = ros.fit_resample(np.array(train_texts).reshape(-1, 1), train_labels)\n",
        "#train_texts_resampled = train_texts_resampled.flatten().tolist()  # Flatten back to list format\n",
        "\n",
        "class_weights = compute_class_weight(\n",
        "    class_weight=\"balanced\",\n",
        "    classes=np.unique(train_labels),\n",
        "    y=train_labels\n",
        ")\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# Modify the model's loss function\n",
        "model.config.problem_type = \"classification\"\n",
        "model.config.num_labels = len(np.unique(train_labels))\n",
        "loss_fn = CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# Subclass Trainer to use the custom loss function\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\").to(device)  # Ensure labels are on the same device\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss = loss_fn(logits, labels)  # Apply custom loss function\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Step 4: Set a reduced max_length based on max sentence length in data\n",
        "max_length = 60\n",
        "\n",
        "# Encode the tweets using the BERTweet tokenizer with new max_length\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length) #removed train_texts_resampled\n",
        "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "# Split training data for validation\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42       #removed train_texts_resampled, train_labels_resampled\n",
        ")\n",
        "\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=max_length)\n",
        "\n",
        "# Create dataset objects\n",
        "train_dataset = TweetDataset(train_encodings, train_labels)\n",
        "val_dataset = TweetDataset(val_encodings, val_labels)\n",
        "test_dataset = TweetDataset(test_encodings, [0] * len(test_df))  # Dummy labels for test\n",
        "\n",
        "\n",
        "# Update dropout probabilities to reduce overfitting\n",
        "model.config.hidden_dropout_prob = 0.2  # Default is 0.1, increase for stronger regularization\n",
        "model.config.attention_probs_dropout_prob = 0.2  # Default is 0.1, increase for stronger regularization\n",
        "\n",
        "\n",
        "# Step 5: Training arguments with best model saving\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./checkpoints',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    greater_is_better=False,\n",
        "    learning_rate=1e-5,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    evaluation_strategy=\"steps\",  # save every 500 steps\n",
        "    save_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    save_total_limit=3,       # retain only last 3 checkpoints\n",
        "    report_to=\"none\",\n",
        "    save_steps=500,\n",
        "#    early_stopping_patience=3,\n",
        ")\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    \"\"\"\n",
        "    Compute evaluation metrics for the model.\n",
        "\n",
        "    Args:\n",
        "        pred: A PredictionOutput object from the Trainer containing logits and labels.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of computed metrics.\n",
        "    \"\"\"\n",
        "    logits, labels = pred\n",
        "    predictions = logits.argmax(axis=-1)  # Get the predicted class by taking the argmax\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(labels, predictions),\n",
        "        \"f1\": f1_score(labels, predictions, average='weighted'),\n",
        "        \"precision\": precision_score(labels, predictions, average='weighted'),\n",
        "        \"recall\": recall_score(labels, predictions, average='weighted')\n",
        "    }\n",
        "\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics  # If you have a metrics function\n",
        ")\n",
        "\"\"\"\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    loss_function=custom_loss\n",
        ")\n",
        "\"\"\"\n",
        "# Add the F1 score and accuracy callback\n",
        "trainer.add_callback(SaveMetricsCallback())\n",
        "trainer.add_callback(MetricsCallback())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fokTNOiRMui4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Step 7: Save the best model to Google Drive\n",
        "trainer.save_model(best_model_path)  # Save the best model checkpoint to Google Drive\n",
        "\n"
      ],
      "metadata": {
        "id": "CRsEOJETMB2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-O0Jx99UhmI"
      },
      "source": [
        "##   **Stage 5**: Evaluate the Model and get model predictions on the test dataset (2 Points)\n",
        "\n",
        "* Upload the model predictions to kaggle by mapping the sentiment column vlalues from numericals the categorical\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Evaluation Results: {eval_results}\")\n",
        "\n",
        "# Step 9: Make predictions on the test set\n",
        "predictions = trainer.predict(test_dataset)\n",
        "predicted_labels = predictions.predictions.argmax(-1)\n",
        "\n",
        "# Map predicted labels back to sentiment categories\n",
        "sentiment_mapping = {0: 'Extremely Negative', 1: 'Negative', 2: 'Neutral', 3: 'Positive', 4: 'Extremely Positive'}\n",
        "predicted_sentiments = [sentiment_mapping[label] for label in predicted_labels]\n",
        "\n",
        "\n",
        "# Add predictions to the test dataframe\n",
        "test_df['Predicted Sentiment'] = predicted_sentiments\n",
        "\n",
        "# Save predictions to a CSV file\n",
        "test_df[['Predicted Sentiment']].to_csv('./bertweet_predictions.csv', index=False)"
      ],
      "metadata": {
        "id": "k1fGvgaXEoOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# prompt: create a classification_report for the outputs.\n",
        "# class_names = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
        "# report = classification_report(train_df['Sentiment'], predicted_sentiments, target_names=class_names)\n",
        "\n",
        "\n",
        "class_names = ['Extremely Negative', 'Negative', 'Neutral', 'Positive', 'Extremely Positive']\n",
        "report = classification_report(train_df['Sentiment'], predicted_sentiments, target_names=class_names)\n",
        "report"
      ],
      "metadata": {
        "id": "5EaqdTz3Vmwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(val_labels, predicted_labels, target_names=class_names))"
      ],
      "metadata": {
        "id": "W0rvwep1LWeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ... (Your existing code for training and prediction) ...\n",
        "\n",
        "\n",
        "# Load the best saved model\n",
        "best_model_path = \"/content/drive/MyDrive/Cohort6_Team5_Hackathon1_CoronavirusClassification_Models/BERTweet_best_model\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(best_model_path)\n",
        "\n",
        "# Generate test_id starting from 1\n",
        "test_ids = np.arange(1, len(test_df) + 1)  # Create a sequence from 1 to the number of rows in test_df\n",
        "\n",
        "# Map predicted labels to sentiment categories (same as before)\n",
        "predicted_sentiments = [sentiment_mapping[label] for label in predicted_labels]\n",
        "\n",
        "# Create submission file with generated 'test_id' and 'Sentiment' columns\n",
        "submission_df = pd.DataFrame({'Test_Id': test_ids, 'Sentiment': predicted_sentiments})\n",
        "submission_df.to_csv('submission.csv', index=False)\n",
        "\n",
        "# Upload to Kaggle (using Kaggle API)\n",
        "!kaggle competitions submit -c to-classify-coronavirus-tweets-during-covid-19 -f submission.csv -m \"Predictions\""
      ],
      "metadata": {
        "id": "Dz1-Bs4pUdsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PSaAlhGGUitF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKnc1WZk9cIk"
      },
      "source": [
        "### Instructions for preparing Kaggle competition predictions\n",
        "\n",
        "\n",
        "* Get the predictions using trained model and prepare a csv file\n",
        "    * DeepNet model gives output for each class, consider the maximum value among all classes as prediction using `np.argmax`.\n",
        "\n",
        "* Predictions (csv) file should contain 2 columns as Sample_Submission.csv\n",
        "  - First column is the Test_Id which is considered as index\n",
        "  - Second column is prediction in decoded form (for eg. Positive, Negative etc...)."
      ]
    }
  ]
}